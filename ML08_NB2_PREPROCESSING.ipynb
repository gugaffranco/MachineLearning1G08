{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2308a1",
   "metadata": {},
   "source": [
    "<h1 style = \"color : #0EE071; text-align : center;\"><em>Nata Project</em> - Data Preprocessing Notebook</h1>\n",
    "<p style = \"font-size : 16px; text-align: center;\">This notebook has the funcion of preparing the <code>Nata_Files/learn.csv</code> dataset for the learning model.</p>\n",
    "<br>\n",
    "<p style = \"font-size : 12px; text-align: center;\"><b>NOVA IMS</b></p>\n",
    "<p style = \"font-size : 10px; text-align: center;\">Machine Learning I</p>\n",
    "<p style = \"font-size : 10px; text-align: center;\">Diogo Gonçalves, João Marques, Juan Mendes, Gustavo Franco & Lucas Casimiro</p>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcebc443",
   "metadata": {},
   "source": [
    "CENAS PA NAO ESQUECER NO 1 NOTEBOOK\n",
    "- mostrar quais as strings unicas pas colunas origin e pastry_type\n",
    "- outliers\n",
    "- missing values\n",
    "- datatypes das colunas (nao corrigi nenhum pq ns se e preciso)\n",
    "- fazer o iqr pa identificar outliers (ainda n limpei nenhum q n os obvios por causa disso)\n",
    "- aprender a tratar da skewness tp log transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5963182",
   "metadata": {},
   "source": [
    "<h2  style = \"color : #0EE071;\"> Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e40ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2cb3d9",
   "metadata": {},
   "source": [
    "<hr style = \"border: 3px solid #0EE071;\">\n",
    "<h2 style = \"color : #0EE071;\">Dataset Reading </h2>\n",
    "<p style = \"font-size : 15px;\">Reading of dataset from <code>learn.csv</code> file, equal to the last document, and making a copy where we will preprocess the data, so we can preserve the original dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe373589",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data = pd.read_csv(\"Nata_files/learn.csv\", sep = \",\", index_col = 0)\n",
    "learn_data_copy = learn_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d075a9",
   "metadata": {},
   "source": [
    "<hr style = \"border: 3px solid #0EE071;\">\n",
    "<h2 style = \"color : #0EE071;\">Data Cleaning </h2>\n",
    "<p style = \"font-size : 15px;\">Handling any outliers, missing values or troublesome data points identified in NB1.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ff13f",
   "metadata": {},
   "source": [
    "First, converting 'origin' and 'pastry_type' values to standardized texts (e.g.,turning 'Lisboa', 'LISBOA', 'lisboa' into the same value), which was recognized as a problem in NB1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66260aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pastel Nata', nan, 'Pastel de Nata', 'Pastel de nata',\n",
       "       'Pastel De Nata'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_data_copy['pastry_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deaa9525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning every string in 'origin' and 'pastry_type' columns to lowercase and stripping whitespace\n",
    "learn_data_copy['origin'] = learn_data_copy['origin'].astype(str).str.lower().str.strip()\n",
    "learn_data_copy['pastry_type'] = learn_data_copy['pastry_type'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Solving the problem \n",
    "learn_data_copy['pastry_type'] = learn_data_copy['pastry_type'].replace({'pastel nata': 'pastel de nata',\n",
    "                                                                         'nan': np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462a732",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #0EE071;\">Handling Outliers</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747315d1",
   "metadata": {},
   "source": [
    "Starting by removing the physically impossible values identified in NB1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cfebe7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting these values to NaN instead of removing the rows, to avoid losing too much data\n",
    "\n",
    "# Sugar content cannot exceed 100g per 100g\n",
    "learn_data_copy.loc[learn_data_copy['sugar_content'] > 100, 'sugar_content'] = np.nan\n",
    "\n",
    "# Fat percentage cannot exceed 100%\n",
    "learn_data_copy.loc[learn_data_copy['cream_fat_content'] > 100, 'cream_fat_content'] = np.nan\n",
    "\n",
    "# Salt > 100g per kg is inedible \n",
    "learn_data_copy.loc[learn_data_copy['salt_ratio'] > 100, 'salt_ratio'] = np.nan\n",
    "\n",
    "# Eggs cook at ~65C. 170C or 575C is impossible for raw egg addition\n",
    "learn_data_copy.loc[learn_data_copy['egg_temperature'] > 100, 'egg_temperature'] = np.nan\n",
    "\n",
    "# Oven/Final temp > 500C is likely an error \n",
    "learn_data_copy.loc[learn_data_copy['final_temperature'] > 500, 'final_temperature'] = np.nan\n",
    "learn_data_copy.loc[learn_data_copy['oven_temperature'] > 500, 'oven_temperature'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbde924",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #0EE071;\">Handling Missing Values</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ca1cb",
   "metadata": {},
   "source": [
    "As seen in NB1, the 'notes_baker' column has no values, so it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca188eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data_copy.drop('notes_baker', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfda395",
   "metadata": {},
   "source": [
    "As seen in the correlation heatmap in NB1, we see 'oven_temperature' and 'final_temperature' have an extremely high correlation, which means having both is redundant, so 'final_temperature' will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05dd7fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data_copy.drop('final_temperature', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725947ac",
   "metadata": {},
   "source": [
    "There is a missing value in the 'quality_class' column, which is the target variable, so we will drop that whole row as it won't be used for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b669155",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data_copy.dropna(subset=['quality_class'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31daec5a",
   "metadata": {},
   "source": [
    "For the remaining columns, we will use either mean or median for numerical columns (depending on skewness), and use the mode for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "746dddd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambient_humidity - Skewness: 0.015582878844407105\n",
      " - Missing values filled using mean\n",
      "baking_duration - Skewness: 1.5361731965648864\n",
      " - Missing values filled using median\n",
      "cooling_period - Skewness: 0.31479598704826645\n",
      " - Missing values filled using mean\n",
      "cream_fat_content - Skewness: -0.8264821399325748\n",
      " - Missing values filled using median\n",
      "egg_temperature - Skewness: -0.0215523837906362\n",
      " - Missing values filled using mean\n",
      "egg_yolk_count - Skewness: 0.5498558732641039\n",
      " - Missing values filled using median\n",
      "lemon_zest_ph - Skewness: 0.33604261530135693\n",
      " - Missing values filled using mean\n",
      "oven_temperature - Skewness: -0.06195219451506645\n",
      " - Missing values filled using mean\n",
      "preheating_time - Skewness: 1.6861674628892802\n",
      " - Missing values filled using median\n",
      "salt_ratio - Skewness: 0.8337125217634596\n",
      " - Missing values filled using median\n",
      "sugar_content - Skewness: 1.307228097089835\n",
      " - Missing values filled using median\n",
      "vanilla_extract - Skewness: 1.6408803255644309\n",
      " - Missing values filled using median\n"
     ]
    }
   ],
   "source": [
    "# Identifying for numerical columns\n",
    "num_cols = learn_data_copy.select_dtypes(include=['float64'])\n",
    "\n",
    "# Using mean or median to fill missing values in numerical columns based on skewness\n",
    "for col in num_cols:\n",
    "    skewness = learn_data_copy[col].skew()\n",
    "\n",
    "    # If skewness is between -0.5 and 0.5, we consider it low skewness\n",
    "    if abs(skewness) < 0.5:\n",
    "        # If skewness is low, use mean\n",
    "        method = 'mean'\n",
    "        learn_data_copy[col] = learn_data_copy[col].fillna(learn_data_copy[col].mean())\n",
    "\n",
    "    else:\n",
    "        # If skewness is high, use median\n",
    "        method = 'median'\n",
    "        learn_data_copy[col] = learn_data_copy[col].fillna(learn_data_copy[col].median())\n",
    "\n",
    "#Clarifying which method was used for each column, along with the skewness value\n",
    "    print(f'{col} - Skewness: {skewness}')\n",
    "    print(f' - Missing values filled using {method}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94dce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the mode to fill missing values in categorical columns\n",
    "cat_cols = ['origin', 'pastry_type']\n",
    "for col in cat_cols:\n",
    "    learn_data_copy[col] = learn_data_copy[col].fillna(learn_data_copy[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ea622",
   "metadata": {},
   "source": [
    "<hr style = \"border: 3px solid #0EE071;\">\n",
    "<h2 style = \"color : #0EE071;\">Encoding categorical variables </h2>\n",
    "<p style = \"font-size : 15px;\">Turning the values of the columns 'origin' and 'quality_class' into numbers, so they can be used in the model.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20a467",
   "metadata": {},
   "source": [
    "For the 'origin' column (feature), since there are only 2 values after the standardization ('lisboa' and 'porto'), we will use One-Hot Encoding (Binary Encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_first=True creates just one column instead of two\n",
    "learn_data_copy = pd.get_dummies(data=learn_data_copy, columns = ['origin'], drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a8203",
   "metadata": {},
   "source": [
    "For the 'quality_class' columns, as it is a target variable, we will use Label Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually mapping it to ensure '1' is the positive class 'OK'\n",
    "learn_data_copy['quality_class'] = learn_data_copy['quality_class'].map({'OK': 1, 'KO' : 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab52167",
   "metadata": {},
   "source": [
    "<hr style = \"border: 3px solid #0EE071;\">\n",
    "<h2 style = \"color : #0EE071;\">Data Partitioning</h2>\n",
    "<p style = \"font-size : 15px;\">Before we perform any scaling, we must split our dataset. This is crucial to prevent <b>Data Leakage</b>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2645d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Class Distribution:\n",
      "quality_class\n",
      "1.0    0.635008\n",
      "0.0    0.364992\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test Set Class Distribution:\n",
      "quality_class\n",
      "1.0    0.635577\n",
      "0.0    0.364423\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define X (features) and y (target)\n",
    "X = learn_data_copy.drop('quality_class', axis=1)\n",
    "y = learn_data_copy['quality_class']\n",
    "\n",
    "# Split into training and testing sets, using stratify to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Checking if the dataset is balanced in both sets\n",
    "print(\"Training Set Class Distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nTest Set Class Distribution:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10d3e7",
   "metadata": {},
   "source": [
    "Note: We are performing a 2-way split (Train/Test). We will not create a validation set here. Instead, we'll use k-Fold Cross-Validation on the training set during the Modeling and Tuning phases. This ensures we maximize the data available for training while maintaining a robust evaluation protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5536e",
   "metadata": {},
   "source": [
    "<hr style = \"border: 3px solid #0EE071;\">\n",
    "<h2 style = \"color : #0EE071;\">Feature Scaling</h2>\n",
    "<p style = \"font-size : 15px;\">To optimize our model, we must address the different scales and distributions of our data. In this section, we'll apply Standard Scaling to center all numerical variables at a mean of 0 and variance of 1. This prevents features with larger magnitudes from dominating the model's learning process.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "94f0c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only numerical columns for scaling\n",
    "features_to_scale = X.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# Defining the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting only the training data and transforming both \n",
    "X_train[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "X_test[features_to_scale] = scaler.transform(X_test[features_to_scale])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
