{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2308a1",
   "metadata": {},
   "source": [
    "<h1 style = \"color : #0EE071; text-align : center;\"><em>Nata Project</em> - Data Preprocessing Notebook</h1>\n",
    "<p style = \"font-size : 16px; text-align: center;\">This notebook has the funcion of preparing the <code>Nata_Files/learn.csv</code> dataset for the learning model.</p>\n",
    "<br>\n",
    "<p style = \"font-size : 12px; text-align: center;\"><b>NOVA IMS</b></p>\n",
    "<p style = \"font-size : 10px; text-align: center;\">Machine Learning I</p>\n",
    "<p style = \"font-size : 10px; text-align: center;\">Diogo Gonçalves, João Marques, Juan Mendes, Gustavo Franco & Lucas Casimiro</p>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcebc443",
   "metadata": {},
   "source": [
    "CENAS PA NAO ESQUECER NO 1 NOTEBOOK\n",
    "- mostrar quais as strings unicas pas colunas origin e pastry_type\n",
    "- outliers\n",
    "- missing values\n",
    "- datatypes das colunas (nao corrigi nenhum pq ns se e preciso)\n",
    "- fazer o iqr pa identificar outliers (ainda n limpei nenhum q n os obvios por causa disso)\n",
    "- aprender a tratar da skewness tp log transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7ae4e",
   "metadata": {},
   "source": [
    "\n",
    "## <a class=\"anchor\" id=\"0th-bullet\">Table of Contents</a>\n",
    "\n",
    "\n",
    "* [<b>1. Importing the dataset and needed libraries</b>](#1st-bullet)<br>\n",
    "    \n",
    "    \n",
    "* [<b>2. Initial Data Cleaning</b>](#2nd-bullet)<br>\n",
    "    * [2.1 Standardizing texts](#3rd-bullet)<br>\n",
    "    * [2.2 Target Handling and Dropping Irrelevant Columns](#4th-bullet)<br>\n",
    "    * [2.3 Outlier \"Masking\"](#5th-bullet)<br>\n",
    "\n",
    "\n",
    "* [<b>3. Data Partitioning</b>](#16th-bullet)<br>\n",
    "\n",
    "\n",
    "* [<b>4. Data Imputation</b>](#16th-bullet)<br>\n",
    "\n",
    "\n",
    "* [<b>5. Encoding Categorical Variables</b>](#16th-bullet)<br>\n",
    "\n",
    "\n",
    "* [<b>6. Exports</b>](#16th-bullet)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5963182",
   "metadata": {},
   "source": [
    "<h2  style = \"color : #0EE071;\"> 1. Importing the dataset and needed libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e40ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e46914",
   "metadata": {},
   "source": [
    "Reading the dataset and making a copy to work on, instead of altering the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe373589",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data = pd.read_csv(\"Nata_files/learn.csv\", sep = \",\", index_col = 0)\n",
    "learn_data_copy = learn_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d075a9",
   "metadata": {},
   "source": [
    "<hr style = \"border: 3px solid #0EE071;\">\n",
    "<h2 style = \"color : #0EE071;\">2. Initial Data Cleaning </h2>\n",
    "<p style = \"font-size : 15px;\">Handling any outliers or troublesome data points identified in NB1.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ff13f",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #0EE071;\">2.1 Standardizing texts</h3>\n",
    "\n",
    "First, converting 'origin' and 'pastry_type' values to standardized texts (e.g.,turning 'Lisboa', 'LISBOA', 'lisboa' into the same value), which was recognized as a problem in NB1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deaa9525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning every string in 'origin' and 'pastry_type' columns to lowercase and stripping whitespace\n",
    "learn_data_copy['origin'] = learn_data_copy['origin'].astype(str).str.lower().str.strip()\n",
    "learn_data_copy['pastry_type'] = learn_data_copy['pastry_type'].astype(str).str.lower().str.strip()\n",
    "\n",
    "# Solving the problem \n",
    "learn_data_copy['pastry_type'] = learn_data_copy['pastry_type'].replace({'pastel nata': 'pastel de nata',\n",
    "                                                                         'nan': np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbde924",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #0EE071;\">2.2 Target Handling and Dropping Irrelevant Columns </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725947ac",
   "metadata": {},
   "source": [
    "There is a missing value in the 'quality_class' column, which is the target variable, so we will drop that whole row as it won't be used for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b669155",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data_copy.dropna(subset=['quality_class'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0ca1cb",
   "metadata": {},
   "source": [
    "As seen in NB1, the 'notes_baker' column has no values, so it will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca188eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_data_copy.drop('notes_baker', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462a732",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #0EE071;\">2.3 Outlier \"Masking\"</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747315d1",
   "metadata": {},
   "source": [
    "Starting by replacing the physically impossible values identified in NB1 as NaN, so they can be imputed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfebe7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting these values to NaN instead of removing the rows, to avoid losing too much data\n",
    "\n",
    "# Sugar content cannot exceed 100g per 100g\n",
    "learn_data_copy.loc[learn_data_copy['sugar_content'] > 100, 'sugar_content'] = np.nan\n",
    "\n",
    "# Fat percentage cannot exceed 100%\n",
    "learn_data_copy.loc[learn_data_copy['cream_fat_content'] > 100, 'cream_fat_content'] = np.nan\n",
    "\n",
    "# Salt > 100g per kg is inedible \n",
    "learn_data_copy.loc[learn_data_copy['salt_ratio'] > 100, 'salt_ratio'] = np.nan\n",
    "\n",
    "# Eggs cook at ~65C. 170C or 575C is impossible for raw egg addition\n",
    "learn_data_copy.loc[learn_data_copy['egg_temperature'] > 100, 'egg_temperature'] = np.nan\n",
    "\n",
    "# Oven/Final temp > 500C is likely an error \n",
    "learn_data_copy.loc[learn_data_copy['final_temperature'] > 500, 'final_temperature'] = np.nan\n",
    "learn_data_copy.loc[learn_data_copy['oven_temperature'] > 500, 'oven_temperature'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab52167",
   "metadata": {},
   "source": [
    "<hr style = \"border: 3px solid #0EE071;\">\n",
    "<h2 style = \"color : #0EE071;\">3. Data Partitioning</h2>\n",
    "<p style = \"font-size : 15px;\">Before we perform any scaling or make, we must split our dataset. This is crucial to prevent <b>Data Leakage</b>. We will also use Label Encoding on the target variable in this section, so the stratify method is applied to these values.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2645d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Class Distribution:\n",
      "quality_class\n",
      "1    0.635008\n",
      "0    0.364992\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test Set Class Distribution:\n",
      "quality_class\n",
      "1    0.635577\n",
      "0    0.364423\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define X (features) and y (target)\n",
    "# Manually mapping the target variable to ensure '1' is the positive class 'OK'\n",
    "X = learn_data_copy.drop('quality_class', axis=1)\n",
    "y = learn_data_copy['quality_class'].map({'OK': 1, 'KO' : 0})\n",
    "\n",
    "# Split into training and testing sets, using stratify to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Checking if the dataset is balanced in both sets\n",
    "print(\"Training Set Class Distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nTest Set Class Distribution:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c10d3e7",
   "metadata": {},
   "source": [
    "Note: We are performing a 2-way split (Train/Test). We will not create a validation set here. Instead, we'll use k-Fold Cross-Validation on the training set during the Modeling and Tuning phases. This ensures we maximize the data available for training while maintaining a robust evaluation protocol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd7a01",
   "metadata": {},
   "source": [
    "<hr style = \"border: 3px solid #0EE071;\">\n",
    "<h2 style = \"color : #0EE071;\">4. Data Imputation</h2>\n",
    "<p style = \"font-size : 15px;\">To deal with missing values, we will use either mean or median for numerical columns (depending on skewness), and use the mode for categorical variables.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "746dddd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ambient_humidity - Skewness: 0.02024732522559659\n",
      " - Missing values filled using mean\n",
      "baking_duration - Skewness: 1.5538202343798468\n",
      " - Missing values filled using median\n",
      "cooling_period - Skewness: 0.3068104917121074\n",
      " - Missing values filled using mean\n",
      "cream_fat_content - Skewness: -0.6072301918569015\n",
      " - Missing values filled using median\n",
      "egg_temperature - Skewness: -0.02416030035571224\n",
      " - Missing values filled using mean\n",
      "egg_yolk_count - Skewness: 0.5270916354134263\n",
      " - Missing values filled using median\n",
      "final_temperature - Skewness: -0.05417159399359049\n",
      " - Missing values filled using mean\n",
      "lemon_zest_ph - Skewness: 0.3401727412742592\n",
      " - Missing values filled using mean\n",
      "oven_temperature - Skewness: -0.06937961360152138\n",
      " - Missing values filled using mean\n",
      "preheating_time - Skewness: 1.627639439365064\n",
      " - Missing values filled using median\n",
      "salt_ratio - Skewness: 0.7822309783435281\n",
      " - Missing values filled using median\n",
      "sugar_content - Skewness: 1.005713226847118\n",
      " - Missing values filled using median\n",
      "vanilla_extract - Skewness: 1.7154876983866194\n",
      " - Missing values filled using median\n"
     ]
    }
   ],
   "source": [
    "# Identifying for numerical columns\n",
    "num_cols = X_train.select_dtypes(include=['float64'])\n",
    "\n",
    "# Using mean or median to fill missing values in numerical columns based on skewness\n",
    "for col in num_cols:\n",
    "    skewness = X_train[col].skew()\n",
    "\n",
    "    # If skewness is between -0.5 and 0.5, we consider it low skewness\n",
    "    if abs(skewness) < 0.5:\n",
    "        # If skewness is low, use mean\n",
    "        method = 'mean'\n",
    "        X_train[col] = X_train[col].fillna(X_train[col].mean())\n",
    "        X_test[col] = X_test[col].fillna(X_train[col].mean())\n",
    "\n",
    "    else:\n",
    "        # If skewness is high, use median\n",
    "        method = 'median'\n",
    "        X_train[col] = X_train[col].fillna(X_train[col].median())\n",
    "        X_test[col] = X_test[col].fillna(X_train[col].median())\n",
    "\n",
    "#Clarifying which method was used for each column, along with the skewness value\n",
    "    print(f'{col} - Skewness: {skewness}')\n",
    "    print(f' - Missing values filled using {method}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94dce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the mode to fill missing values in categorical columns\n",
    "cat_cols = ['origin', 'pastry_type']\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].fillna(X_train[col].mode()[0])\n",
    "    X_test[col] = X_test[col].fillna(X_train[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ea622",
   "metadata": {},
   "source": [
    "<hr style = \"border: 3px solid #0EE071;\">\n",
    "<h2 style = \"color : #0EE071;\">5. Encoding categorical variables </h2>\n",
    "<p style = \"font-size : 15px;\">Turning the values of the categorical columns into numbers. In this case, we only have the 'origin' column (feature), and since there are only 2 values after the standardization ('lisboa' and 'porto'), we will use One-Hot Encoding (Binary Encoding), and can use .get_dummies.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3719dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_first=True creates just one column instead of two\n",
    "X_train = pd.get_dummies(data=X_train, columns = ['origin'], drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020dad2",
   "metadata": {},
   "source": [
    "<hr style = \"border: 3px solid #0EE071;\">\n",
    "<h2 style = \"color : #0EE071;\">6. Exports</h2>\n",
    "<p style = \"font-size : 15px;\">Exporting the training and testing datasets, so they can be used in the next notebooks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9918de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving to the 'data_clean' folder inside 'Nata_files'\n",
    "X_train.to_pickle('Nata_files/data_clean/X_train_clean.pkl')\n",
    "X_test.to_pickle('Nata_files/data_clean/X_test_clean.pkl')\n",
    "y_train.to_pickle('Nata_files/data_clean/y_train_clean.pkl')\n",
    "y_test.to_pickle('Nata_files/data_clean/y_test_clean.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
